# Adaptive Testing & Debugging Sandbox â€” MVP Flow Draft

## Executive Summary

This project is like a practice arena for microservices.

**Phase 0.1 (Live Observation)**: We watch how services normally talk to each other and record those interactions.

**Phase 0.2 (Sandbox Replay)**: We replay the same traffic in a safe environment, inject one failure (like delay or dropped request), and see how the system reacts.

ðŸ‘‰ These phases don't run at the same time. First we capture and learn (Phase 0.1), then we replay and test (Phase 0.2).

## Analogy for Beginner Developers

Think of it like debugging a multiplayer game:

In **Phase 0.1**, you just watch how players move and interact in real matches. You write down what "normal" looks like (e.g., most players move at speed X).

In **Phase 0.2**, you replay the same match in a training mode, but this time you add an obstacle or slow one player down to see how the whole game reacts.

## High-Level Flow

**Phase 0.1: Capture â†’ Baseline â†’ Detect â†’ Notify â†’ Transition â†’ â†’ â†’ Phase 0.2: Replay â†’ Inject â†’ Detect â†’ Notify**

```mermaid
graph TD
    Start([Start MVP]) --> Phase1[Phase 0.1: Live Observation]
    
    subgraph "Phase 0.1 --- Watch + Learn"
        Phase1 --> Capture[ðŸ“¹ Capture Real Traffic]
        Capture --> Analyze[ðŸ” Analyze Patterns]
        Analyze --> Baseline[ðŸ“Š Build Normal Baseline]
        Baseline --> Detect[ðŸš¨ Detect Anomalies]
    end
    
    Detect --> Wait[â¸ï¸ Phase Transition]
    Wait --> Phase2[Phase 0.2: Sandbox Testing]
    
    subgraph "Phase 0.2 - Test Safely"
        Phase2 --> Setup[ðŸ—ï¸ Setup Sandbox Environment]
        Setup --> Replay[â–¶ï¸ Replay Captured Traffic]
        Replay --> Inject[ðŸ’‰ Inject One Failure]
        Inject --> Watch[ðŸ‘€ Watch System React]
        Watch --> Report[ðŸ“ Generate Report]
        Report --> Notify[ðŸ“² Send Slack Email]
    end
    
    Notify --> End([End])
    
    style Phase1 fill:#e1f5fe
    style Phase2 fill:#fff3e0
```

## Step-by-Step Flow

### Phase 0.1: Live Observation

1. **Capture traffic** â€” Use OpenTelemetry to record real requests between services.
2. **Build a baseline** â€” Measure what's "normal" (e.g., Service B usually replies in 200ms with <2% errors).
3. **Detect changes** â€” If future runs show slower replies or higher error rates, we flag it.
4. **Notify** â€” Send a Slack/email with a short summary of what changed.

### Phase 0.2: Sandbox Replay

1. **Replay traffic** â€” Re-run the same captured requests in a safe test environment.
2. **Inject one failure** â€” Add either artificial delay or drop a request using Toxiproxy.
3. **Observe effects** â€” Measure how the system reacts under failure.
4. **Notify** â€” Send another alert/report explaining which service failed first and by how much.

## Tech Stack (MVP)

- **OpenTelemetry + Collector** â†’ the â€œmicrophones and video cameraâ€ ðŸŽ¤ðŸ“¹

    - Captures every conversation between services (A â†’ B â†’ C).

    - The Collector exports those traces into a neat JSON/NDJSON file (the â€œrecordingâ€) so we can replay later.

- **Jaeger** â†’ the â€œvideo playerâ€ ðŸŽ¬

    - Stores and visualizes those service traces.

    - Lets us rewind and see: â€œOh, Service B was 200ms slower here.â€

- **Prometheus + Grafana** â†’ the â€œscoreboard and scoreboard screenâ€ ðŸ“Š

    - Prometheus = collects metrics (how fast, how many errors).

    - Grafana = dashboard to make those numbers look understandable (charts, alerts).

- **Python (asyncio + httpx)** â†’ the â€œreplay engineâ€ â–¶ï¸

    - Reads the captured JSON bundle.

    - Replays requests in the same order/timing as the original run.

    - Easy for us devs to write quickly (asyncio for timing, httpx for HTTP).

- **Toxiproxy** â†’ the â€œmischief remote controlâ€ ðŸ’‰

    - Lets us break one thing on purpose (add delay, drop a request).

    - Example: â€œMake Service B 200ms slower this run.â€

- **Docker Compose** â†’ the â€œsandbox builderâ€ ðŸ—ï¸

    - Spins up local copies of Jaeger, Prometheus, Grafana, Toxiproxy, and our services in one simple config file.

    - Lighter weight than Kubernetes â€” faster for MVP development.

- **Slack Webhooks** â†’ the â€œalarm bellâ€ ðŸš¨

    - When anomalies are detected, send a short message straight to Slack:

    - Example: â€œâš ï¸ Service B latency +40% (220ms â†’ 310ms).â€

- **JSON/YAML files** â†’ the â€œnotebooks and rulebooksâ€ ðŸ“’

    - Capture bundles (what we recorded), replay reports, and failure injection configs all live here.

    - Example: capture_001.json, replay_run_007.json, failure.yaml.

    - Makes tests reproducible without needing a database.


## Simple BDD Conditions

### When Things Are Normal:
- **GIVEN** Service A usually responds in 50-100ms
- **WHEN** Service A responds in 75ms
- **THEN** Mark as "âœ… Normal"

### When Things Go Wrong:
- **GIVEN** Service B normally handles 100 requests/minute
- **WHEN** Service B suddenly drops to 10 requests/minute
- **THEN** Send alert: "âš ï¸ Service B is really slow!"

### During Testing:
- **GIVEN** We're in sandbox mode with fake traffic
- **WHEN** We make Database connection 50% slower
- **THEN** Measure if checkout still works or times out

## Simple Testing Rules

- If p95 latency increases by 20%, send alert.
- If error rate increases by >2%, send alert.
- Always flag the first failing service in the chain.

## Why It Matters

Instead of waiting for production outages, this sandbox helps us:

- Understand what "normal" looks like in our system.
- Test failures safely before real users are impacted.
- Find weak spots early, saving debugging time and avoiding downtime.
- This is like building a safety net: we first watch the real system, then practice breaking it in a safe copy so we know how to react before real users are impacted.
