#!/usr/bin/env python3
"""
Report Generator - Compares baseline and failure YAML files and generates summary reports.

Usage:
    python scripts/report_generator.py \
        --baseline data/baselines/normal_baseline.yaml \
        --compare data/baselines/failure_vs_normal.yaml \
        --output reports/summary_report.md
"""

import argparse
import yaml
from pathlib import Path
from datetime import datetime


def load_baseline(filepath):
    """Load baseline YAML file."""
    with open(filepath, 'r') as f:
        return yaml.safe_load(f)


def calculate_diff(baseline_val, compare_val):
    """Calculate percentage difference between two values."""
    if baseline_val == 0:
        return 0 if compare_val == 0 else float('inf')
    return ((compare_val - baseline_val) / baseline_val) * 100


def generate_markdown_report(baseline_data, compare_data, baseline_name, compare_name):
    """Generate a Markdown formatted report."""
    report = []
    report.append("# Performance Comparison Report")
    report.append(f"\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"\n**Baseline:** `{baseline_name}`")
    report.append(f"**Comparison:** `{compare_name}`")
    report.append("\n---\n")
    
    # Summary table
    report.append("## Summary")
    report.append("\n| Metric | Baseline | Comparison | Difference | % Change |")
    report.append("|--------|----------|------------|------------|----------|")
    
    metrics = ['p50_ms', 'p95_ms', 'error_rate']
    for metric in metrics:
        baseline_val = baseline_data.get(metric, 0)
        compare_val = compare_data.get(metric, 0)
        diff = compare_val - baseline_val
        pct_change = calculate_diff(baseline_val, compare_val)
        
        # Format values based on metric type
        if metric == 'error_rate':
            baseline_str = f"{baseline_val:.4f}"
            compare_str = f"{compare_val:.4f}"
            diff_str = f"{diff:+.4f}"
        else:
            baseline_str = f"{baseline_val:.2f}"
            compare_str = f"{compare_val:.2f}"
            diff_str = f"{diff:+.2f}"
        
        if pct_change == float('inf'):
            pct_str = "‚àû"
        else:
            pct_str = f"{pct_change:+.2f}%"
        
        report.append(f"| {metric} | {baseline_str} | {compare_str} | {diff_str} | {pct_str} |")
    
    # Analysis section
    report.append("\n## Analysis\n")
    
    p50_change = calculate_diff(baseline_data.get('p50_ms', 0), compare_data.get('p50_ms', 0))
    p95_change = calculate_diff(baseline_data.get('p95_ms', 0), compare_data.get('p95_ms', 0))
    error_change = calculate_diff(baseline_data.get('error_rate', 0), compare_data.get('error_rate', 0))
    
    if p50_change > 10:
        report.append(f"‚ö†Ô∏è **Warning:** p50 latency increased by {p50_change:.2f}%")
    elif p50_change < -10:
        report.append(f"‚úÖ **Improvement:** p50 latency decreased by {abs(p50_change):.2f}%")
    else:
        report.append(f"‚ÑπÔ∏è p50 latency change is within acceptable range ({p50_change:.2f}%)")
    
    if p95_change > 10:
        report.append(f"\n‚ö†Ô∏è **Warning:** p95 latency increased by {p95_change:.2f}%")
    elif p95_change < -10:
        report.append(f"\n‚úÖ **Improvement:** p95 latency decreased by {abs(p95_change):.2f}%")
    else:
        report.append(f"\n‚ÑπÔ∏è p95 latency change is within acceptable range ({p95_change:.2f}%)")
    
    if error_change > 5:
        report.append(f"\nüö® **Critical:** Error rate increased by {error_change:.2f}%")
    elif error_change < -5:
        report.append(f"\n‚úÖ **Improvement:** Error rate decreased by {abs(error_change):.2f}%")
    else:
        report.append(f"\n‚ÑπÔ∏è Error rate change is within acceptable range ({error_change:.2f}%)")
    
    report.append("\n---\n")
    report.append("*Report generated by AI Adaptive Sandbox*")
    
    return "\n".join(report)


def generate_html_report(baseline_data, compare_data, baseline_name, compare_name):
    """Generate an HTML formatted report."""
    html = []
    html.append("<!DOCTYPE html>")
    html.append("<html><head>")
    html.append("<title>Performance Comparison Report</title>")
    html.append("<style>")
    html.append("body { font-family: Arial, sans-serif; margin: 40px; }")
    html.append("table { border-collapse: collapse; width: 100%; margin: 20px 0; }")
    html.append("th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }")
    html.append("th { background-color: #4CAF50; color: white; }")
    html.append("tr:nth-child(even) { background-color: #f2f2f2; }")
    html.append(".warning { color: #ff9800; }")
    html.append(".critical { color: #f44336; }")
    html.append(".improvement { color: #4CAF50; }")
    html.append("</style>")
    html.append("</head><body>")
    html.append("<h1>Performance Comparison Report</h1>")
    html.append(f"<p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>")
    html.append(f"<p><strong>Baseline:</strong> {baseline_name}</p>")
    html.append(f"<p><strong>Comparison:</strong> {compare_name}</p>")
    html.append("<h2>Summary</h2>")
    html.append("<table>")
    html.append("<tr><th>Metric</th><th>Baseline</th><th>Comparison</th><th>Difference</th><th>% Change</th></tr>")
    
    metrics = ['p50_ms', 'p95_ms', 'error_rate']
    for metric in metrics:
        baseline_val = baseline_data.get(metric, 0)
        compare_val = compare_data.get(metric, 0)
        diff = compare_val - baseline_val
        pct_change = calculate_diff(baseline_val, compare_val)
        
        if metric == 'error_rate':
            baseline_str = f"{baseline_val:.4f}"
            compare_str = f"{compare_val:.4f}"
            diff_str = f"{diff:+.4f}"
        else:
            baseline_str = f"{baseline_val:.2f}"
            compare_str = f"{compare_val:.2f}"
            diff_str = f"{diff:+.2f}"
        
        pct_str = "‚àû" if pct_change == float('inf') else f"{pct_change:+.2f}%"
        
        html.append(f"<tr><td>{metric}</td><td>{baseline_str}</td><td>{compare_str}</td><td>{diff_str}</td><td>{pct_str}</td></tr>")
    
    html.append("</table>")
    html.append("</body></html>")
    
    return "\n".join(html)


def main():
    parser = argparse.ArgumentParser(description='Generate performance comparison reports')
    parser.add_argument('--baseline', required=True, help='Path to baseline YAML file')
    parser.add_argument('--compare', required=True, help='Path to comparison YAML file')
    parser.add_argument('--output', required=True, help='Output report file path')
    parser.add_argument('--format', choices=['markdown', 'html'], default='markdown', 
                        help='Output format (default: markdown)')
    
    args = parser.parse_args()
    
    # Load data
    baseline_data = load_baseline(args.baseline)
    compare_data = load_baseline(args.compare)
    
    # Generate report
    if args.format == 'html':
        report = generate_html_report(
            baseline_data, compare_data,
            Path(args.baseline).name, Path(args.compare).name
        )
    else:
        report = generate_markdown_report(
            baseline_data, compare_data,
            Path(args.baseline).name, Path(args.compare).name
        )
    
    # Write report
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(report)
    
    print(f"‚úÖ Report generated: {output_path}")
    print(f"\nPreview:\n{'-' * 60}")
    print(report[:500] + "..." if len(report) > 500 else report)


if __name__ == '__main__':
    main()
