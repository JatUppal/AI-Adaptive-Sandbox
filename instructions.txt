Goal
Expose a tiny service that:

1. pulls the latest metrics from Prometheus,
2. uses the trained model to compute a risk score,
3. publishes failure_risk_score{service="service-a"} to Prometheus,
4. adds a Grafana gauge & line panel.

What you'll build
1. services/ai-predictor/app.py (FastAPI)
2. services/ai-predictor/requirements.txt
3. Prometheus "push" (or pull) exposition endpoint: /metrics
4. Grafana panels (update JSON and docs)

Service behavior
1. /health → {"ok": true}
2. /predict → returns JSON:
{
  "service": "service-a",
  "risk_score": 0.83,
  "status": "HIGH_RISK",
  "reason": "High p95 & err_rate"
}
3. /metrics → Expose failure_risk_score{service="service-a"} <float>

Steps
1. Create services/ai-predictor/requirements.txt
    fastapi
    uvicorn
    requests
    prometheus_client
    scikit-learn
    joblib

2. Implement services/ai-predictor/app.py
- Load model & feature columns at startup from models/.
- On /predict:
    - Query Prometheus (same 1-min window features used in training).
    - Build feature vector → model.predict_proba → risk.
    - Compute status from thresholds (e.g. ≥0.8 HIGH, ≥0.5 MEDIUM).
    - Return JSON.
- On /metrics:
    - Set a Gauge: failure_risk_score{service="service-a"}.
    -Each prediction also updates the gauge.
3. Dockerize service (optional now; can run locally first)
4. Add to docker-compose.yml (optional if you containerize in this phase)

ai-predictor:
  build:
    context: ./services/ai-predictor
  image: ai-predictor
  container_name: ai-predictor
  environment:
    PROM_URL: http://prometheus:9090
  ports:
    - "8085:8080"
  depends_on: [prometheus]

5. Configure Grafana
    - Add a new Gauge panel bound to Prometheus query:
        failure_risk_score{service="service-a"}
    - Add a Time series panel for the same metric.
    - Thresholds: 0.5 (yellow), 0.8 (red).
    - Save dashboard JSON back to observability/grafana-dashboards/reporting_dashboard.json.

6. Update README
Add instructions on how to run the predictor and see the gauge.

Commands (local run)
# install deps
pip install -r services/ai-predictor/requirements.txt

# run service
uvicorn services.ai-predictor.app:app --reload --port 8085

# test prediction
curl -s http://localhost:8085/predict | jq

# view metric
curl -s http://localhost:8085/metrics | grep failure_risk_score

Acceptance criteria

/predict returns a JSON with risk_score in [0,1]

/metrics exposes failure_risk_score{service="service-a"}

Grafana has a gauge + time series showing the risk (updates within ~15s)

README updated with startup instructions